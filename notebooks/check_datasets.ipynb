{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e49ec6",
   "metadata": {},
   "source": [
    "# Check datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_jsonl_datasets(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            p_data = json.loads(line)\n",
    "            data.append(p_data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nq_dev_merge = open_jsonl_datasets(path='../datasets/NQ-open.dev.merged.jsonl')\n",
    "\n",
    "len(data_nq_dev_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nq_dev_merge[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6831e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nq_train = open_jsonl_datasets(path='../datasets/NQ-open.train.jsonl')\n",
    "len(data_nq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42295d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nq_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_qa = pd.read_csv('../datasets/simple_qa_test_set_with_documents.csv')\n",
    "\n",
    "df_simple_qa.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "deabbf35",
=======
   "id": "59765ee6",
>>>>>>> 334c358a3b93e9f0c68e3079230b95a324d448b7
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def open_gzip_jsonl_data(path):\n",
    "    data = []\n",
    "    with gzip.open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            _ = json.loads(line)\n",
    "            data.append(_)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "2e60d920",
=======
   "id": "ebe9713d",
>>>>>>> 334c358a3b93e9f0c68e3079230b95a324d448b7
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_original = open_gzip_jsonl_data(path='/disk-1/drezov/nq_original/train/nq-train-00.jsonl.gz')\n",
    "\n",
    "nq_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "c624f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nq_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e7ed3",
=======
   "id": "d01472fb",
>>>>>>> 334c358a3b93e9f0c68e3079230b95a324d448b7
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_original[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "11a02e39",
=======
   "id": "ef0403d6",
>>>>>>> 334c358a3b93e9f0c68e3079230b95a324d448b7
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
<<<<<<< HEAD
    "model_name = 'answerdotai/ModernBERT-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f86965",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_original[0]['question_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_original[0]['document_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nq_original[0]['document_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer = nq_original[0]['annotations'][0]['long_answer']\n",
    "long_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer['content'] = []\n",
    "\n",
    "for token_dict in nq_original[0]['document_tokens'][long_answer['start_token']:long_answer['end_token']]:\n",
    "    if not(token_dict['html_token']):\n",
    "        long_answer['content'].append(token_dict['token'])\n",
    "\n",
    "\n",
    "long_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers = nq_original[0]['annotations'][0]['short_answers']\n",
    "short_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d367ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "async def document_cleaning(document_tokens: List[Dict]):\n",
    "    clean_document = []\n",
    "    for token_dict in document_tokens:\n",
    "        if not(token_dict['html_token']):\n",
    "            clean_document.append(token_dict['token'])\n",
    "    return clean_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048408c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_document = await document_cleaning(document_tokens=nq_original[0]['document_tokens'])\n",
    "clean_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34965e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(clean_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_original[0]['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nq_original[0]['long_answer_candidates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = '/disk-1/drezov/nq_original/train'\n",
    "list_files = os.listdir(dir)\n",
    "len(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_processing(document: Dict):\n",
    "    answers = []\n",
    "    for annotation in document['annotations']:\n",
    "        long_answer = annotation['long_answer']\n",
    "        long_answer['content'] = []\n",
    "        for token_dict in document['document_tokens'][long_answer['start_token']:long_answer['end_token']]:\n",
    "            if not(token_dict['html_token']):\n",
    "                long_answer['content'].append(token_dict['token'])\n",
    "        short_answers = [ _ for _ in annotation['short_answers'] ]\n",
    "        for short_answer in short_answers:\n",
    "            short_answer['content'] = []\n",
    "            for token_dict in document['document_tokens'][short_answer['start_token']:short_answer['end_token']]:\n",
    "                if not(token_dict['html_token']):\n",
    "                    short_answer['content'].append(token_dict['token'])\n",
    "        answers.append({'long_answer': long_answer, 'short_answers': short_answers})\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = await answer_processing(nq_original[0])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def documents_processing(nq_batch : List[Dict]):\n",
    "    preprocessed_documents = []\n",
    "    for doc in nq_batch:\n",
    "        preprocessed_doc = {}\n",
    "        if doc['document_tokens'] and doc['question_tokens'] and doc['annotations']:\n",
    "            context = await document_cleaning(doc['document_tokens'])\n",
    "            preprocessed_doc['question'] = doc['question_tokens']\n",
    "            preprocessed_doc['context'] = context\n",
    "            answers = await answer_processing(doc)\n",
    "            preprocessed_doc['answers'] = answers\n",
    "            preprocessed_documents.append(preprocessed_doc)\n",
    "\n",
    "    return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03325",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def nq_processing(dir : str = '/disk-1/drezov/nq_original/train'):\n",
    "    list_files = os.listdir(dir)\n",
    "    all_documents = []\n",
    "    for file_name in list_files:\n",
    "        file_path = os.path.join(dir, file_name)\n",
    "        nq_batch = open_gzip_jsonl_data(file_path)\n",
    "        documents = await documents_processing(nq_batch)\n",
    "        all_documents.append(documents)\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(\n",
    "    filepath: str = None,\n",
    "    encoding: str = 'utf-8',\n",
    "    data: List[Dict] = None\n",
    "    ):\n",
    "    with open(filepath, 'w', encoding=encoding) as f:\n",
    "        for idx, item in enumerate(data):\n",
    "            json_line = json.dumps(item)\n",
    "            f.write(json_line + '\\n')\n",
    "    print(f'Dataset save complete, path: {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75757719",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = await nq_processing()\n",
    "save_jsonl(data=all_documents, filepath='/disk-1/drezov/preprocessed_data/nq_original/train/train_nq.jsonl')\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b62bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import orjson\n",
    "\n",
    "path = '/disk-1/drezov/preprocessed_data/nq_original/train/train_nq.jsonl'\n",
    "data = []\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    stop_f = 100\n",
    "    data = []\n",
    "    for line in f:\n",
    "        _ = orjson.loads(line)\n",
    "        data.append(_)\n",
    "        stop_f -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8eef603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48ef9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995b146",
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "model_name = 'Qwen/Qwen3-1.7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
>>>>>>> 334c358a3b93e9f0c68e3079230b95a324d448b7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm_fine_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
